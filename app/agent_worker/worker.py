# app/agent_worker/worker.py
import asyncio
import logging

from livekit.agents import JobContext, WorkerOptions, cli, JobProcess
from livekit.agents.llm import ChatContext, ChatMessage
from livekit.agents.voice_assistant import VoiceAssistant
from livekit.plugins import deepgram, silero, cartesia, openai

from core.config import Settings



settings = Settings()

# Configure basic logging
logging.basicConfig(level=logging.INFO)

def prewarm(proc: JobProcess):
    """
    Pre-warms resources before the worker starts accepting jobs.
    This function is called once per process, and it's a good place to load
    models or other resources that are shared across all jobs.
    """
    logging.info("Pre-warming VAD model...")
    try:
        proc.userdata["vad"] = silero.VAD.load()
        logging.info("VAD model loaded successfully.")
    except Exception as e:
        logging.error(f"Failed to load VAD model: {e}")
        # Exit if critical resources can't be loaded
        exit(1)


async def entrypoint(ctx: JobContext):
    """
    This is the entrypoint for each new agent job.
    A new job is created for each new call that the agent joins.
    """
    # Get the agent's persona/prompt from the job data, with a fallback.
    # This data is sent from our `livekit_service.py`
    prompt = ctx.job.data or "You are a friendly and helpful voice assistant."
    logging.info(f"Starting agent with prompt: {prompt}")

    # Create the initial chat context with the system prompt (the agent's persona)
    initial_ctx = ChatContext(
        messages=[
            ChatMessage(
                role="system",
                content=prompt,
            )
        ]
    )

    # Initialize the VoiceAssistant with our chosen STT, LLM, and TTS plugins
    assistant = VoiceAssistant(
        vad=ctx.proc.userdata["vad"],
        stt=deepgram.STT(model="nova-2", language="multi", api_key=settings.DEEPGRAM_API_KEY),
        llm=openai.LLM(model="gpt-4o-mini", api_key=settings.OPENAI_API_KEY),
        tts=cartesia.TTS(model="sonic-2", voice="f786b574-daa5-4673-aa0c-cbe3e8534c02", api_key=settings.CARTESIA_API_KEY),
        chat_ctx=initial_ctx,
    )

    
    await ctx.connect()
    logging.info(f"Agent connected to room: {ctx.room.name}")

    # Start the assistant's processing loop
    assistant.start(ctx.room)

    # Wait a moment for the connection to be fully established
    await asyncio.sleep(1)

    # Have the assistant say an initial greeting to the user
    # This will be generated by the LLM based on the initial system prompt.
    logging.info("Generating initial greeting...")
    await assistant.say("Greet the user and ask how you can help.", allow_interruptions=True)


if __name__ == "__main__":
    # This is the main entry point for the worker process.
    # It sets up the worker options, including the entrypoint and pre-warming functions.
    cli.run_app(WorkerOptions(entrypoint_fnc=entrypoint, prewarm_fnc=prewarm))